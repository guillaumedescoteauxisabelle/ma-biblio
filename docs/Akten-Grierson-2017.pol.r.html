<h1 id="real-time-interactive-sequence-generation-and-control-with-recurrent-neural-network-ensembles">Real-time interactive sequence generation and control with Recurrent Neural Network ensembles</h1>
<p>Akten</p>
<p>Citer: (Akten &amp; Grierson, 2017)</p>
<p>FTag: Akten-Grierson-2017</p>
<p>APA7: Akten, M., &amp; Grierson, M. (2017). Real-time interactive sequence generation and control with Recurrent Neural Network ensembles. _ArXiv:1612.04687 [Cs] _. <a href="http://arxiv.org/abs/1612.04687">http://arxiv.org/abs/1612.04687</a></p>
<p>Recurrent Neural Networks (RNN)</p>
<p>Long Short Term Memory (LSTM) RNNs, are a popular and very successful method for learning and generating sequences</p>
<p> [...] current generative RNN techniques do not allow real-time interactive control of the sequence generation process, thus aren’t well suited for live creative expression .</p>
<p>#problematic</p>
<p>We propose a method of real-time continuous control and ‘steering’ of sequence generation using an ensemble of RNNs and dynamically altering the mixture weights of the models. W</p>
<p>e demonstrate the method using character based LSTM networks and a gestural interface allowing users to ‘conduct’ the generation of text.</p>
<p>Recurrent Neural Networks (RNN) are artificial neural networks with recurrent connections, allowing them to learn temporal regularities and model sequences.</p>
<p>Long Short Term Memory (LSTM) [ 16] is a recurrent architecture that overcomes the problem of gradients exponentially vanishing [ 15, 1] , and allows RNNs to be trained many time-steps into the past, to learn more complex programs [ 21].</p>
<h1 id="references">References</h1>
<p> [1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult.IEEE Transactions on Neural Networks, 5(2):157–166, 1994.</p>
<p> [2] N. Boulanger-Lewandowski, P. Vincent, and Y. Bengio. Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription.arXiv preprintarXiv:1206.6392, 2012.</p>
<p> [3] F. Chollet. Keras: Deep Learning library for Theano and TensorFlow, 2015.</p>
<p> [4] L. Crnkovic-friis and L. Crnkovic-friis. Generative Choreography using Deep Learning.arXiv preprintarXiv:1605.06921, 2016.</p>
<p> [5] T. G. Dietterich. Ensemble Methods in Machine Learning.MCS ’00: Proceedings of the First InternationalWorkshop on Multiple Classifier Systems, pages 1–15, 2000.</p>
<p> [6] D. Eck and J. Schmidhuber. A First Look at Music Composition using LSTM Recurrent Neural Networks.Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale, 103, 2002.</p>
<p>\ [7] F. A. Gers and J. Schmidhuber. Recurrent nets that time and count. InNeural Networks, 2000. IJCNN2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, pages 189–194. IEEE,2000.</p>
<p>\ [8] R. Goodwin. Word Synth, 2016. </p>
<p>\ [9] A. Graves. Sequence transduction with recurrent neural networks.arXiv preprint arXiv:1211.3711, 2012. </p>
<p>\ [10] A. Graves. Generating sequences with Recurrent Neural Networks.arXiv preprint arXiv:1308.0850, 2013. </p>
<p>\ [11] A. Graves, M. Liwicki, S. Fernández, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel connectionistsystem for unconstrained handwriting recognition.IEEE Transactions on Pattern Analysis and MachineIntelligence, 31(5):855–868, 2009. </p>
<p>\ [12] K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, and J. Schmidhuber. LSTM: A Search SpaceOdyssey.arXiv preprint arXiv:1503.04069, page 10, 2015. </p>
<p>\ [13] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. DRAW: A Recurrent Neural Network For ImageGeneration.arXiv preprint arXiv:1502.04623, 2015. </p>
<p>\ [14] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,T. N. Sainath, and B. Kingsbury. Deep Neural Networks for Acoustic Modeling in Speech Recognition.IEEE Signal Processing Magazine, 29(6):82–97, 2012. </p>
<p>\ [15] S. Hochreiter.Untersuchungen zu dynamischen neuronalen Netzen. PhD thesis, Technische UniversitätMünchen, 1991. </p>
<p>\ [16] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory.Neural Computation, 9(8):1735–1780,1997. </p>
<p>\ [17] A. Karpathy. The Unreasonable Effectiveness of Recurrent Neural Networks, 2015. </p>
<p>\ [18] Z. Lieberman, T. Watson, and A. Castro. OpenFrameworks, 2016. </p>
<p>\ [19] A. Nayebi and M. Vitelli. GRUV : Algorithmic Music Generation using Recurrent Neural Networks. 2015. </p>
<p>\ [20] V. Pham, T. Bluche, C. Kermorvant, and J. Louradour. Dropout Improves Recurrent Neural Networks forHandwriting Recognition. InFrontiers in Handwriting Recognition (ICFHR), 2014 14th InternationalConference on, pages 285–290. IEEE, 2014. </p>
<p>\ [21] J. Schmidhuber. Deep Learning in Neural Networks: An Overview.Neural Networks, 61:85–117, 2015. </p>
<p>\ [22] B. Sturm. Recurrent Neural Networks for Folk Music Generation, 2015. </p>
<p>\ [23] I. Sutskever.Training Recurrent neural Networks. PhD thesis, University of Toronto, 2013. </p>
<p>\ [24] I. Sutskever, J. Martens, and G. Hinton. Generating Text with Recurrent Neural Networks. InProceedingsof the 28th International Conference on Machine Learning (ICML-11), pages 1017–1024, 2011. </p>
<p>\ [25] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to Sequence Learning with Neural Networks. InAdvancesin Neural Information Processing Systems (NIPS), pages 3104–3112, 2014. </p>
<p>\ [26] The Theano Development Team. Theano: A Python framework for fast computation of mathematicalexpressions, 2016. </p>
<p>\ [27] M. Wright and A. Freed. Open Sound Control: A new protocol for communicating with sound synthesizers.InProceedings of the 1997 International Computer Music Conference (ICMC), 1997. </p>
<p>\ [28] Z. Wu and S. King. Investigating Gated Recurrent Neural Networks for Speech Synthesis. InIEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5140–5144. IEEE,2016. </p>
<p>\ [29] W. Zaremba and I. Sutskever. Learning to Execute. In2nd International Conference on LearningRepresentations (ICLR2014), 2014.</p>
<p>#biblio</p>
<p> [...] with increased compute power and large training sets, LSTMs and related architectures are proving successful not only in sequence classification [11, 14, 20, 12] , but also in sequence generation in many domains such as music [6, 2, 19, 22] , text [24, 23] , handwriting [10] , images [13] , machine translation \ [25] , speech synthesis \ [28] and even choreography \ [4] </p>
<p>#review</p>
<p> [...] most current applications of sequence generation with RNNs is not a real-time, interactive process.</p>
<p>#problematic</p>
<p>ll does not provide real-time continuous control in the manner required for the creation of expressive interface</p>
<p>#problematic</p>
<hr>
<hr>
