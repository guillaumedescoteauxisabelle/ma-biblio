<h1 id="a-style-aware-content-loss-for-real-time-hd-style-transfer">A Style-Aware Content Loss for Real-time HD Style Transfer</h1>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="http://zotero.org/users/180474/items/AR8HXJRW">ZotWeb</a></td>
<td>article-journal</td>
<td></td>
</tr>
<tr>
<td><a href="http://arxiv.org/abs/1807.10201">Src Url</a></td>
<td>[[Sanakoyeu]], [[Kotovenko]], [[Lang]], [[Ommer]] (2018)</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="abstract">Abstract</h2>
<p>Recently, style transfer has received a lot of attention. While much of this research has aimed at speeding up processing, the approaches are still lacking from a principled, art historical standpoint: a style is more than just a single image or an artist, but previous work is limited to only a single instance of a style or shows no benefit from more images. Moreover, previous work has relied on a direct comparison of art in the domain of RGB images or on CNNs pre-trained on ImageNet, which requires millions of labeled object bounding boxes and can introduce an extra bias, since it has been assembled without artistic consideration. To circumvent these issues, we propose a style-aware content loss, which is trained jointly with a deep encoder-decoder network for real-time, high-resolution stylization of images and videos. We propose a quantitative measure for evaluating the quality of a stylized image and also have art historians rank patches from our approach against those from previous work. These and our qualitative results ranging from small image patches to megapixel stylistic images and videos show that our approach better captures the subtle nature in which a style affects content.</p>
<hr>
<h2 id="annotations">Annotations</h2>
<p>A Style-Aware Content Loss for Real-time HD Style Transfer</p>
<p>&lt;font size=-3&gt;Citer: (Sanakoyeu et al., 2018)<br><br>FTag: Sanakoyeu-et-al-2018<br><br>APA7: Sanakoyeu, A., Kotovenko, D., Lang, S., &amp; Ommer, B. (2018). A Style-Aware Content Loss for Real-time HD Style Transfer. _ArXiv:1807.10201 [Cs] _. <a href="http://arxiv.org/abs/1807.10201">http://arxiv.org/abs/1807.10201</a>&lt;/font&gt;



</p>
<p>style transfer has received a lot of attent</p>
<p> [...] much of this research has aimed at speeding up processing, the approaches are still lacking from a principled, art historical standpoint: a style is more than just a single image or an artist, but previous work is limited to only a single instance of a style or shows no benefit from more image.  (Sanakoyeu et al., 2018)<br>[[NSTProblematic]] </p>
<p> [...] pre-trained on ImageNet, [...] can introduce an extra bias, since it has been assembled without artistic consideration.  (Sanakoyeu et al., 2018)<br>[[NSTProblematic]] </p>
<p>To circumvent these issues, we propose a <strong>style-aware content loss</strong>, which is trained jointly with a deep encoder-decoder network for real-time, high-resolution stylization of images and video</p>
<p>Style transfer, generative network, deep learning</p>
<p>#keywords<br>[[NSTKeywords]] </p>
<p>Project page: <a href="https://compvis.github.io/adaptive-style-transfer">https://compvis.github.io/adaptive-style-transfer</a> .<br>[[NSTReferences]] </p>
<h2 id="extra-qualitative-results">Extra Qualitative Results</h2>
<p>Altering the style of an existing artwork Our method is able to change the style of an existing artwork, rendering it in another style. It means, that our algorithm can also handle content images which are artistic, which, to our knowledge, was never shown in previous work before. We refer the reader to the results on the project page.</p>
<p><img src="15yTyjzg8QNRaNys8Kwi.png" alt=""></p>
<hr>
<hr>
