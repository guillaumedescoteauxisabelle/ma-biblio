<hr>
<h2 id="fiche-cr-e-par-guillaume-d-isabelle-2020-">Fiche créée par Guillaume D.Isabelle, 2020 </h2>
<h3 id="hashtagged">HashTagged</h3>
<p><img src="f2714547-7987-4e33-8f16-60f4edefc687" alt=""></p>
<hr>
<hr>
<h1 id="image-to-image-translation-with-conditional-adversarial-networks">Image-To-Image Translation With Conditional Adversarial Networks</h1>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="http://zotero.org/users/180474/items/58J68RE6">ZotWeb</a></td>
<td>paper-conference</td>
<td></td>
</tr>
<tr>
<td><a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html">Src Url</a></td>
<td>[[Isola]], [[Zhu]], [[Zhou]], [[Efros]] (2017)</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="abstract">Abstract</h2>
<p>undefined</p>
<hr>
<h2 id="annotations">Annotations</h2>
<p>Image-to-Image Translation with Conditional Adversarial Networks</p>
<p>&lt;font size=-3&gt;Citer: (Isola et al., 2017)<br><br>FTag: Isola-et-al-2017<br><br>APA7: Isola, P., Zhu, J.-Y., Zhou, T., &amp; Efros, A. A. (2017). <em>Image-To-Image Translation With Conditional Adversarial Networks</em>. 1125–1134. <a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html">https://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html</a>&lt;/font&gt;



</p>
<p>Avec cette librairie, il est possible de faire la translation d&#39;une image entrante vers une image sortante. Il est Intéressant de voir que nous lui donnons le résultat désiré en sortie ce qui correspond à une dynamique structurale intéressante relativement au processus créatif.  </p>
<blockquote>
<p>In analogy to automatic language translation, we define automatic image-to-image translation as the task of translating one possible representation of a scene into another, given sufficient training data.(Isola et l.,2018, p.1)<br>[[ref2011210911]] | [[GAN]] | [[ArticleAkten]] </p>
</blockquote>
<blockquote>
<p>CNNs learn to minimize a loss function [...]   a lot of manual effort still goes into designing effective losses. [...]   tell the CNN what we wish it to minimize.</p>
</blockquote>
<p>#problematic</p>
<blockquote>
<p>It would be highly desirable if we could instead specify only a high-level goal, like “make the output indistinguishable from reality”, and then automatically learn a loss function appropriate for satisfying this goal. Fortunately, this is exactly what is done by the recently proposed Generative Adversarial Networks (GANs) [ 22 , 12 , 41 , 49 , 59 ]   (Isola et al., 2017, p.1126)<br>[[AIConcept]] | [[CreativeAI]] </p>
</blockquote>
<p>Le réseau génératif adverse (RGA) sont des modèles génératifs qui apprennent une cartographie à partir d&#39;un vecteur de bruit aléatoire vers une image tandit que le réseau génératif adverse conditionel (RGAC) apprend sa cartographie à partir d&#39;observation d&#39;image et de bruit aléatoire.</p>
<p>Je ne comprend pas encore pleinement la chose.</p>
<blockquote>
<p>GANs are generative models that learn a mapping from random noise vector z to output image y , G : z → y [ 22 ]. In contrast, conditional GANs learn a mapping from observed image x and random noise vector z , to y , G : { x, z } → y . The generator G is trained to produce outputs that cannot be distinguished from “real” images by an adversarially trained discriminator, D , which is trained to do as well as possible at detecting the generator’s “fakes”. This training procedure is diagrammed in Figure 2<br>[[GAN.def]] | [[RGA]] | [[RGAC]] | [[ref2011271354]] </p>
</blockquote>
<p><img src="1QHZ2Wa75d6YxCWE2NY4.png" alt=""></p>
<p><img src="121pTYHaQxrwsDD4CZSH.png" alt=""></p>
<p>Figure 2.  </p>
<hr>
<hr>
<h3 id="section-analyse-structur-e-en-grille-sagrid-">Section analyse structurée en grille (SAGrid)</h3>
