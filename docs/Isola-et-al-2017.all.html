<hr>
<h2 id="fiche-cr-e-par-guillaume-d-isabelle-2020-">Fiche créée par Guillaume D.Isabelle, 2020 </h2>
<h3 id="hashtagged">HashTagged</h3>
<h5 id="section-carte-de-lecture-polaire">Section carte de lecture polaire</h5>
<p><img src="f2714547-7987-4e33-8f16-60f4edefc687" alt=""></p>
<hr>
<hr>
<h5 id="section-annotation-polaire">Section annotation polaire</h5>
<p>Image-to-Image Translation with Conditional Adversarial Networks</p>
<p>Citer: (Isola et al., 2017)</p>
<p>FTag: Isola-et-al-2017</p>
<p>APA7: Isola, P., Zhu, J.-Y., Zhou, T., &amp; Efros, A. A. (2017). <em>Image-To-Image Translation With Conditional Adversarial Networks</em>. 1125–1134. <a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html">https://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html</a></p>
<p>Avec cette librairie, il est possible de faire la translation d&#39;une image entrante vers une image sortante. Il est Intéressant de voir que nous lui donnons le résultat désiré en sortie ce qui correspond à une dynamique structurale intéressante relativement au processus créatif.  </p>
<blockquote>
<p>In analogy to automatic language translation, we define automatic image-to-image translation as the task of translating one possible representation of a scene into another, given sufficient training data.(Isola et l.,2018, p.1)</p>
</blockquote>
<blockquote>
<p>CNNs learn to minimize a loss function [...]   a lot of manual effort still goes into designing effective losses. [...]   tell the CNN what we wish it to minimize.</p>
</blockquote>
<p>#problematic</p>
<blockquote>
<p>It would be highly desirable if we could instead specify only a high-level goal, like “make the output indistinguishable from reality”, and then automatically learn a loss function appropriate for satisfying this goal. Fortunately, this is exactly what is done by the recently proposed Generative Adversarial Networks (GANs) [ 22 , 12 , 41 , 49 , 59 ]   (Isola et al., 2017, p.1126)</p>
</blockquote>
<p>Le réseau génératif adverse (RGA) sont des modèles génératifs qui apprennent une cartographie à partir d&#39;un vecteur de bruit aléatoire vers une image tandit que le réseau génératif adverse conditionel (RGAC) apprend sa cartographie à partir d&#39;observation d&#39;image et de bruit aléatoire.</p>
<p>Je ne comprend pas encore pleinement la chose.</p>
<blockquote>
<p>GANs are generative models that learn a mapping from random noise vector z to output image y , G : z → y [ 22 ]. In contrast, conditional GANs learn a mapping from observed image x and random noise vector z , to y , G : { x, z } → y . The generator G is trained to produce outputs that cannot be distinguished from “real” images by an adversarially trained discriminator, D , which is trained to do as well as possible at detecting the generator’s “fakes”. This training procedure is diagrammed in Figure 2</p>
</blockquote>
<p><img src="1QHZ2Wa75d6YxCWE2NY4.png" alt=""></p>
<p><img src="121pTYHaQxrwsDD4CZSH.png" alt=""></p>
<p>Figure 2.  </p>
<hr>
<hr>
<h3 id="section-analyse-structur-e-en-grille-sagrid-">Section analyse structurée en grille (SAGrid)</h3>
