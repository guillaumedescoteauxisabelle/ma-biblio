{"csl":{"id":"http://zotero.org/users/180474/items/XBAEC42P","type":"article-journal","container-title":"ACM Transactions on Interactive Intelligent Systems","DOI":"10.1145/2643204","ISSN":"2160-6455, 2160-6463","issue":"4","journalAbbreviation":"ACM Trans. Interact. Intell. Syst.","language":"en","note":"ZSCC: NoCitationData[s1]","page":"1-34","source":"DOI.org (Crossref)","title":"Adaptive Gesture Recognition with Variation Estimation for Interactive Systems","URL":"https://dl.acm.org/doi/10.1145/2643204","volume":"4","author":[{"family":"Caramiaux","given":"Baptiste"},{"family":"Montecchio","given":"Nicola"},{"family":"Tanaka","given":"Atau"},{"family":"Bevilacqua","given":"Frédéric"}],"accessed":{"date-parts":[["2021",2,11]]},"issued":{"date-parts":[["2015",1,28]]}},"citer":"Citer:(Caramiaux et al., 2015)\n\nFTag: Caramiaux-et-al-2015\n\nAPA7: Caramiaux, B., Montecchio, N., Tanaka, A., & Bevilacqua, F. (2015). Adaptive Gesture Recognition with Variation Estimation for Interactive Systems. _ACM Transactions on Interactive Intelligent Systems_, _4_(4), 1–34. [https://doi.org/10.1145/2643204](https://doi.org/10.1145/2643204)\n\n [Polar](https://app.getpolarized.io/doc/1dVJNgCTU5push7YNVgsLE43j7ZzDdBCUPjiCsTLXorLDsCNiU) -","annotations":{"0":{"type":"text","rendermd":"Adaptive Gesture Recognition with Variation Estimation for Interactive Systems\n==============================================================================","id":"1JAY5m7Fux","created":"2021-02-11T16:43:12.429Z","color":"yellow","guid":"1PtyXyQ758"},"1":{"type":"text","rendermd":"<font size=-3>Citer:(Caramiaux et al., 2015)\n\nFTag: Caramiaux-et-al-2015\n\nAPA7: Caramiaux, B., Montecchio, N., Tanaka, A., & Bevilacqua, F. (2015). Adaptive Gesture Recognition with Variation Estimation for Interactive Systems. _ACM Transactions on Interactive Intelligent Systems_, _4_(4), 1–34. [https://doi.org/10.1145/2643204](https://doi.org/10.1145/2643204)\n\n [Polar](https://app.getpolarized.io/doc/1dVJNgCTU5push7YNVgsLE43j7ZzDdBCUPjiCsTLXorLDsCNiU) -</font>","id":"12SbcVqRc1","created":"2021-02-11T16:43:20.905Z","color":"yellow","guid":"1qGeiQydhU"},"2":{"type":"text","rendermd":"This paper presents a gesture recognition/adaptation system for Human Computer Interaction","id":"1g1dFNNZD1","created":"2021-02-11T16:43:38.743Z","color":"yellow","guid":"1g3hanf7yB"},"3":{"type":"text","rendermd":"These systems are fundamentally not designed to take into account variations that occur during the movement performanc","id":"1dbk2WcQ8Q","created":"2021-04-09T14:09:06.727Z","color":"#FF6900","guid":"12tPnCNXVw"},"4":{"type":"image","data":{"width":513,"height":748,"id":"11DhE6zmezi5NX95TuA4","fname":"11DhE6zmezi5NX95TuA4.png"},"rendermd":"![](11DhE6zmezi5NX95TuA4.png)","id":"1PHChFNokr","created":"2021-04-09T14:12:54.981Z","guid":"1PHChFNokr"},"5":{"type":"text","rendermd":"4.1. Continuous state model","id":"1fx8ZEPLt6","created":"2021-04-09T14:19:01.693Z","color":"#9900EF","guid":"1fx8ZEPLt6"},"6":{"type":"text","rendermd":"4.2. State space model","id":"12VbRoPyV1","created":"2021-04-09T14:19:09.313Z","color":"#9900EF","guid":"12VbRoPyV1"},"7":{"type":"image","data":{"width":667,"height":553,"id":"1wRbjQmRPCnzLAohnajS","fname":"1wRbjQmRPCnzLAohnajS.png"},"rendermd":"![](1wRbjQmRPCnzLAohnajS.png)","id":"12SBxYU71o","created":"2021-04-09T14:19:35.050Z","guid":"12SBxYU71o"},"8":{"type":"text","rendermd":"Fig. 3 . Gesture vocabulary from [Wobbrock et al. 2007].","id":"12w17U9LU8","created":"2021-04-09T14:19:40.891Z","color":"yellow","guid":"12w17U9LU8"},"9":{"type":"image","data":{"width":1047,"height":602,"id":"128gdzLjApxqyn6dVkUH","fname":"128gdzLjApxqyn6dVkUH.png"},"rendermd":"![](128gdzLjApxqyn6dVkUH.png)","id":"18hNqVmsPf","created":"2021-04-09T14:21:08.103Z","guid":"18hNqVmsPf"},"10":{"type":"text","rendermd":"Fig. 11 . Gesture variations allowed in the application: variations in size, speed and tilt. Each variation is used to control the sound feedback parameter: loudness, playback speed and filter","id":"124tajgWfY","created":"2021-04-09T14:21:29.789Z","color":"yellow","guid":"124tajgWfY"},"11":{"type":"text","rendermd":"CONCLUSION\n==========","id":"12G6F9e4iX","created":"2021-04-09T14:22:18.244Z","color":"yellow","guid":"12qSd5Ys5y"},"12":{"type":"text","rendermd":"r gesture recognition that can adapt and estimate in real-time variations occurring during gesture executio","id":"1ev4K8hucr","created":"2021-04-09T14:22:36.653Z","color":"#9900EF","guid":"1ev4K8hucr"},"13":{"type":"text","rendermd":"ability of our method to track changes in phase, scaling and rotation","id":"1Q4JxtH6Qc","created":"2021-04-09T14:22:48.490Z","color":"green","guid":"1Q4JxtH6Qc"},"14":{"type":"text","rendermd":"> [...] important feature [...] causal inference [...] represents a clear advantage for interactive applications [...] partial results are available during the gestures [...] anticipate which gesture is currently performed by early recognition.","id":"1kAbnsy1AQ","created":"2021-04-09T14:23:54.453Z","color":"red","guid":"12atpmUqCi"}}}