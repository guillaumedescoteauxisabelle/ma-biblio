<hr>
<h2 id="fiche-cr-e-par-guillaume-d-isabelle-2020-">Fiche créée par Guillaume D.Isabelle, 2020 </h2>
<h3 id="hashtagged">HashTagged</h3>
<h1 id="unsupervised-robust-disentangling-of-latent-characteristics-for-image-synthesis">Unsupervised Robust Disentangling of Latent Characteristics for Image Synthesis</h1>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="http://zotero.org/users/180474/items/ENI27QW6">ZotWeb</a></td>
<td>article-journal</td>
<td></td>
</tr>
<tr>
<td><a href="http://arxiv.org/abs/1910.10223">Src Url</a></td>
<td>[[Esser]], [[Haux]], [[Ommer]] (2019)</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="abstract">Abstract</h2>
<p>Deep generative models come with the promise to learn an explainable representation for visual objects that allows image sampling, synthesis, and selective modification. The main challenge is to learn to properly model the independent latent characteristics of an object, especially its appearance and pose. We present a novel approach that learns disentangled representations of these characteristics and explains them individually. Training requires only pairs of images depicting the same object appearance, but no pose annotations. We propose an additional classifier that estimates the minimal amount of regularization required to enforce disentanglement. Thus both representations together can completely explain an image while being independent of each other. Previous methods based on adversarial approaches fail to enforce this independence, while methods based on variational approaches lead to uninformative representations. In experiments on diverse object categories, the approach successfully recombines pose and appearance to reconstruct and retarget novel synthesized images. We achieve significant improvements over state-of-the-art methods which utilize the same level of supervision, and reach performances comparable to those of pose-supervised approaches. However, we can handle the vast body of articulated object classes for which no pose models/annotations are available.</p>
<hr>
<h2 id="annotations">Annotations</h2>
<h1 id="unsupervised-robust-disentangling-of-latent-characteristics-for-image-synthesis">Unsupervised Robust Disentangling of Latent Characteristics for Image Synthesis</h1>
<p>&lt;font size=-3&gt;Citer:(Esser et al., 2019)<br><br>FTag: Esser-et-al-2019<br><br>APA7: Esser, P., Haux, J., &amp; Ommer, B. (2019). Unsupervised Robust Disentangling of Latent Characteristics for Image Synthesis. _ArXiv:1910.10223 [Cs] _. <a href="http://arxiv.org/abs/1910.10223">http://arxiv.org/abs/1910.10223</a>&lt;/font&gt;



</p>
<h1 id="intentions">Intentions</h1>
<p>*  disentanglement ??  Interesting term for the separation of content and style.</p>
<ul>
<li>Possibly to find the optimal NST library<br>[[disentanglement]] | [[ref2012031450]] </li>
</ul>
<p>We propose an additional classifier that estimates the minimal amount of regularization required to enforce disentanglement. Thus both representations to- gether can completely explain an image while being independent of each other. Previous methods based on adversarial approaches fail to enforce this independence, while methods based on variational approaches lead to uninformative representations.<br>[[AIProblematic]] | [[ref2012031450]] | [[DisentanglementIssue]] </p>
<hr>
<hr>
<h3 id="section-analyse-structur-e-en-grille-sagrid-">Section analyse structurée en grille (SAGrid)</h3>
