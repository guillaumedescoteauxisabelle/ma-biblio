{"csl":{"id":"http://zotero.org/users/180474/items/IPA4SDX6","type":"article-journal","abstract":"In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.","container-title":"Neural Networks","DOI":"10.1016/j.neunet.2014.09.003","ISSN":"0893-6080","journalAbbreviation":"Neural Networks","language":"en","note":"ZSCC: 0010963","page":"85-117","source":"ScienceDirect","title":"Deep learning in neural networks: An overview","title-short":"Deep learning in neural networks","URL":"http://www.sciencedirect.com/science/article/pii/S0893608014002135","volume":"61","author":[{"family":"Schmidhuber","given":"Jürgen"}],"accessed":{"date-parts":[["2020",12,16]]},"issued":{"date-parts":[["2015",1,1]]}},"citer":"Citer: (Schmidhuber, 2015)\n\nFTag: Schmidhuber-2015\n\nAPA7: Schmidhuber, J. (2015). Deep learning in neural networks: An overview. _Neural Networks_, _61_, 85–117. [https://doi.org/10.1016/j.neunet.2014.09.003](https://doi.org/10.1016/j.neunet.2014.09.003)\n\n( [Cache](https://jgwill.github.io/www.fichiers/Schmidhuber-2015.cache.html))","annotations":{"0":{"type":"text","rendermd":"Deep learning in neural networks:An overview\n============================================","id":"12owVXqyYW","created":"2020-12-16T06:13:18.383Z","color":"yellow","guid":"12aYGohWib"},"1":{"type":"text","rendermd":"<font size=-3>Citer: (Schmidhuber, 2015)\n\nFTag: Schmidhuber-2015\n\nAPA7: Schmidhuber, J. (2015). Deep learning in neural networks: An overview. _Neural Networks_, _61_, 85–117. [https://doi.org/10.1016/j.neunet.2014.09.003](https://doi.org/10.1016/j.neunet.2014.09.003)\n\n( [Cache](https://jgwill.github.io/www.fichiers/Schmidhuber-2015.cache.html))</font>","id":"1PnQpqmTLT","created":"2020-12-16T06:13:46.103Z","color":"yellow","guid":"1LtNhp7Zdg"},"2":{"type":"text","rendermd":"Abbreviations in alphabetical order\n-----------------------------------\n\nAE:\n\nAutoencoder\n\nAI:\n\nArtificial Intelligence\n\nANN:\n\nArtificial Neural Network\n\nBFGS:\n\nBroyden–Fletcher–Goldfarb–Shanno\n\nBNN:\n\nBiological Neural Network\n\nBM:\n\nBoltzmann Machine\n\nBP:\n\nBackpropagation\n\nBRNN:\n\nBi-directional Recurrent Neural Network\n\nCAP:\n\nCredit Assignment Path\n\nCEC:\n\nConstant Error Carousel\n\nCFL:\n\nContext Free Language\n\nCMA-ES:\n\nCovariance Matrix Estimation ES\n\nCNN:\n\nConvolutional Neural Network\n\nCoSyNE:\n\nCo-Synaptic Neuro-Evolution\n\nCSL:\n\nContext Sensitive Language\n\nCTC:\n\nConnectionist Temporal Classification\n\nDBN:\n\nDeep Belief Network\n\nDCT:\n\nDiscrete Cosine Transform\n\nDL:\n\nDeep Learning\n\nDP:\n\nDynamic Programming\n\nDS:\n\nDirect Policy Search\n\nEA:\n\nEvolutionary Algorithm\n\nEM:\n\nExpectation Maximization\n\nES:\n\nEvolution Strategy\n\nFMS:\n\nFlat Minimum Search\n\nFNN:\n\nFeedforward Neural Network\n\nFSA:\n\nFinite State Automaton\n\nGMDH:\n\nGroup Method of Data Handling\n\nGOFAI:\n\nGood Old-Fashioned AI\n\nGP:\n\nGenetic Programming\n\nGPU:\n\nGraphics Processing Unit\n\nGPU-MPCNN:\n\nGPU-Based MPCNN\n\nHMM:\n\nHidden Markov Model\n\nHRL:\n\nHierarchical Reinforcement Learning\n\nHTM:\n\nHierarchical Temporal Memory\n\nHMAX:\n\nHierarchical Model “and X”\n\nLSTM:\n\nLong Short-Term Memory (RNN)\n\nMDL:\n\nMinimum Description Length\n\nMDP:\n\nMarkov Decision Process\n\nMNIST:\n\nMixed National Institute of Standards and Technology Database\n\nMP:\n\nMax-Pooling\n\nMPCNN:\n\nMax-Pooling CNN\n\nNE:\n\nNeuroEvolution\n\nNEAT:\n\nNE of Augmenting Topologies\n\nNES:\n\nNatural Evolution Strategies\n\nNFQ:\n\nNeural Fitted Q-Learning\n\nNN:\n\nNeural Network\n\nOCR:\n\nOptical Character Recognition\n\nPCC:\n\nPotential Causal Connection\n\nPDCC:\n\nPotential Direct Causal Connection\n\nPM:\n\nPredictability Minimization\n\nPOMDP:\n\nPartially Observable MDP\n\nRAAM:\n\nRecursive Auto-Associative Memory\n\nRBM:\n\nRestricted Boltzmann Machine\n\nReLU:\n\nRectified Linear Unit\n\nRL:\n\nReinforcement Learning\n\nRNN:\n\nRecurrent Neural Network\n\nR-prop:\n\nResilient Backpropagation\n\nSL:\n\nSupervised Learning\n\nSLIM NN:\n\nSelf-Delimiting Neural Network\n\nSOTA:\n\nSelf-Organizing Tree Algorithm\n\nSVM:\n\nSupport Vector Machine\n\nTDNN:\n\nTime-Delay Neural Network\n\nTIMIT:\n\nTI/SRI/MIT Acoustic-Phonetic Continuous Speech Corpus\n\nUL:\n\nUnsupervised Learning\n\nWTA:\n\nWinner-Take-All","id":"12nTQtYS6N","created":"2020-12-16T06:39:41.152Z","color":"yellow","guid":"1YNH5VR3UD"}}}