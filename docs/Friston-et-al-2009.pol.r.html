<h1 id="reinforcement-learning-or-active-inference-">Reinforcement Learning or Active Inference?</h1>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="http://zotero.org/users/180474/items/BYQP2IGB">ZotWeb</a></td>
<td>article-journal</td>
<td></td>
</tr>
<tr>
<td><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0006421">Src Url</a></td>
<td>[[Friston]], [[Daunizeau]], [[Kiebel]] (2009)</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="abstract">Abstract</h2>
<p>This paper questions the need for reinforcement learning or control theory when optimising behaviour. We show that it is fairly simple to teach an agent complicated and adaptive behaviours using a free-energy formulation of perception. In this formulation, agents adjust their internal states and sampling of the environment to minimize their free-energy. Such agents learn causal structure in the environment and sample it in an adaptive and self-supervised fashion. This results in behavioural policies that reproduce those optimised by reinforcement learning and dynamic programming. Critically, we do not need to invoke the notion of reward, value or utility. We illustrate these points by solving a benchmark problem in dynamic programming; namely the mountain-car problem, using active perception or inference under the free-energy principle. The ensuing proof-of-concept may be important because the free-energy formulation furnishes a unified account of both action and perception and may speak to a reappraisal of the role of dopamine in the brain.</p>
<hr>
<h2 id="annotations">Annotations</h2>
<p>Important to limit the goals an agent has. </p>
<blockquote>
<p>We start with the premise that adaptive agents or phenotypes must occupy a limited repertoire of states.<br>[[AIDesign]] | [[ArticleAgent]] | [[AgentDesign]] </p>
</blockquote>
<h1 id="the-free-energy-principle">The free-energy principle</h1>
<h1 id="reinforcement-learning-or-active-inference-">Reinforcement Learning or Active Inference?</h1>
<p>&lt;font size=-3&gt;Citer: (Friston et al., 2009)<br><br>FTag: Friston-et-al-2009<br><br>APA7: Friston, K. J., Daunizeau, J., &amp; Kiebel, S. J. (2009). Reinforcement Learning or Active Inference? <em>PLOS ONE</em>, _4_(7), e6421. <a href="https://doi.org/10.1371/journal.pone.0006421">https://doi.org/10.1371/journal.pone.0006421</a><br><br> <a href="https://app.simplenote.com/p/45MQ5Q">https://app.simplenote.com/p/45MQ5Q</a>&lt;/font&gt;






</p>
<hr>
<hr>
