{"csl":{"id":"http://zotero.org/users/180474/items/7KKSHIJZ","type":"article-journal","abstract":"Recurrent Neural Networks (RNN), particularly Long Short Term Memory (LSTM) RNNs, are a popular and very successful method for learning and generating sequences. However, current generative RNN techniques do not allow real-time interactive control of the sequence generation process, thus aren't well suited for live creative expression. We propose a method of real-time continuous control and 'steering' of sequence generation using an ensemble of RNNs and dynamically altering the mixture weights of the models. We demonstrate the method using character based LSTM networks and a gestural interface allowing users to 'conduct' the generation of text.","container-title":"arXiv:1612.04687 [cs]","note":"ZSCC: NoCitationData[s1] \narXiv: 1612.04687","source":"arXiv.org","title":"Real-time interactive sequence generation and control with Recurrent Neural Network ensembles","URL":"http://arxiv.org/abs/1612.04687","author":[{"family":"Akten","given":"Memo"},{"family":"Grierson","given":"Mick"}],"accessed":{"date-parts":[["2020",10,5]]},"issued":{"date-parts":[["2017",2,9]]}},"annotations":{"0":{"type":"text","rendermd":"Real-time interactive sequence generation and control with Recurrent Neural Network ensembles\n=============================================================================================","id":"1dabnWQ5g6","created":"2020-10-06T01:57:02.516Z","color":"yellow","guid":"1P8mUJzMyA"},"1":{"type":"text","rendermd":"Akten\n\n  \n\nCiter: (Akten & Grierson, 2017)\n\nFTag: Akten-Grierson-2017\n\nAPA7: Akten, M., & Grierson, M. (2017). Real-time interactive sequence generation and control with Recurrent Neural Network ensembles. _ArXiv:1612.04687 [Cs] _. [http://arxiv.org/abs/1612.04687](http://arxiv.org/abs/1612.04687)","id":"12n2nnYjjM","created":"2020-10-06T01:58:01.974Z","color":"yellow","guid":"12MN5Vjoa6"},"2":{"type":"text","rendermd":"Recurrent Neural Networks (RNN)","id":"12JY5dCZvM","created":"2020-10-06T01:58:19.867Z","color":"yellow","guid":"12JY5dCZvM"},"3":{"type":"text","rendermd":"Long Short Term Memory (LSTM) RNNs, are a popular and very successful method for learning and generating sequences","id":"123b1rCALY","created":"2020-10-06T01:59:05.217Z","color":"yellow","guid":"123b1rCALY"},"4":{"type":"text","rendermd":" [...] current generative RNN techniques do not allow real-time interactive control of the sequence generation process, thus aren’t well suited for live creative expression .\n\n  \n\n#problematic","hData":{"t":" [...] current generative RNN techniques do not allow real-time interactive control of the sequence generation process, thus aren’t well suited for live creative expression .\n\n  \n\n","y":"problematic"},"id":"12jafBxbi9","created":"2020-10-06T01:59:19.622Z","color":"#FF6900","guid":"1eXvBxJ5xS"},"5":{"type":"text","rendermd":"We propose a method of real-time continuous control and ‘steering’ of sequence generation using an ensemble of RNNs and dynamically altering the mixture weights of the models. W","id":"1TJoz7o45z","created":"2020-10-06T02:00:37.717Z","color":"#9900EF","guid":"1TJoz7o45z"},"6":{"type":"text","rendermd":"e demonstrate the method using character based LSTM networks and a gestural interface allowing users to ‘conduct’ the generation of text.","id":"1wBZy8ETtQ","created":"2020-10-06T02:00:46.971Z","color":"red","guid":"1wBZy8ETtQ"},"7":{"type":"text","rendermd":"Recurrent Neural Networks (RNN) are artificial neural networks with recurrent connections, allowing them to learn temporal regularities and model sequences.","id":"12djbM5FnS","created":"2020-10-06T02:01:24.645Z","color":"yellow","guid":"12djbM5FnS"},"8":{"type":"text","rendermd":"Long Short Term Memory (LSTM) [ 16] is a recurrent architecture that overcomes the problem of gradients exponentially vanishing [ 15, 1] , and allows RNNs to be trained many time-steps into the past, to learn more complex programs [ 21].","id":"1rmNto8yPR","created":"2020-10-06T02:01:44.209Z","color":"yellow","guid":"1rmNto8yPR"},"9":{"type":"text","rendermd":"References\n==========\n\n  \n\n [1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult.IEEE Transactions on Neural Networks, 5(2):157–166, 1994.\n\n  \n\n [2] N. Boulanger-Lewandowski, P. Vincent, and Y. Bengio. Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription.arXiv preprintarXiv:1206.6392, 2012.\n\n  \n\n [3] F. Chollet. Keras: Deep Learning library for Theano and TensorFlow, 2015.\n\n  \n\n [4] L. Crnkovic-friis and L. Crnkovic-friis. Generative Choreography using Deep Learning.arXiv preprintarXiv:1605.06921, 2016.\n\n  \n\n [5] T. G. Dietterich. Ensemble Methods in Machine Learning.MCS ’00: Proceedings of the First InternationalWorkshop on Multiple Classifier Systems, pages 1–15, 2000.\n\n  \n\n [6] D. Eck and J. Schmidhuber. A First Look at Music Composition using LSTM Recurrent Neural Networks.Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale, 103, 2002.\n\n  \n\n\\ [7\\] F. A. Gers and J. Schmidhuber. Recurrent nets that time and count. InNeural Networks, 2000. IJCNN2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, pages 189–194. IEEE,2000.\n\n  \n\n\\ [8\\] R. Goodwin. Word Synth, 2016. \n\n  \n\n\\ [9\\] A. Graves. Sequence transduction with recurrent neural networks.arXiv preprint arXiv:1211.3711, 2012. \n\n  \n\n\\ [10\\] A. Graves. Generating sequences with Recurrent Neural Networks.arXiv preprint arXiv:1308.0850, 2013. \n\n  \n\n\\ [11\\] A. Graves, M. Liwicki, S. Fernández, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel connectionistsystem for unconstrained handwriting recognition.IEEE Transactions on Pattern Analysis and MachineIntelligence, 31(5):855–868, 2009. \n\n  \n\n\\ [12\\] K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, and J. Schmidhuber. LSTM: A Search SpaceOdyssey.arXiv preprint arXiv:1503.04069, page 10, 2015. \n\n  \n\n\\ [13\\] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. DRAW: A Recurrent Neural Network For ImageGeneration.arXiv preprint arXiv:1502.04623, 2015. \n\n  \n\n\\ [14\\] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,T. N. Sainath, and B. Kingsbury. Deep Neural Networks for Acoustic Modeling in Speech Recognition.IEEE Signal Processing Magazine, 29(6):82–97, 2012. \n\n  \n\n\\ [15\\] S. Hochreiter.Untersuchungen zu dynamischen neuronalen Netzen. PhD thesis, Technische UniversitätMünchen, 1991. \n\n  \n\n\\ [16\\] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory.Neural Computation, 9(8):1735–1780,1997. \n\n  \n\n\\ [17\\] A. Karpathy. The Unreasonable Effectiveness of Recurrent Neural Networks, 2015. \n\n  \n\n\\ [18\\] Z. Lieberman, T. Watson, and A. Castro. OpenFrameworks, 2016. \n\n  \n\n\\ [19\\] A. Nayebi and M. Vitelli. GRUV : Algorithmic Music Generation using Recurrent Neural Networks. 2015. \n\n  \n\n\\ [20\\] V. Pham, T. Bluche, C. Kermorvant, and J. Louradour. Dropout Improves Recurrent Neural Networks forHandwriting Recognition. InFrontiers in Handwriting Recognition (ICFHR), 2014 14th InternationalConference on, pages 285–290. IEEE, 2014. \n\n  \n\n\\ [21\\] J. Schmidhuber. Deep Learning in Neural Networks: An Overview.Neural Networks, 61:85–117, 2015. \n\n  \n\n\\ [22\\] B. Sturm. Recurrent Neural Networks for Folk Music Generation, 2015. \n\n  \n\n\\ [23\\] I. Sutskever.Training Recurrent neural Networks. PhD thesis, University of Toronto, 2013. \n\n  \n\n\\ [24\\] I. Sutskever, J. Martens, and G. Hinton. Generating Text with Recurrent Neural Networks. InProceedingsof the 28th International Conference on Machine Learning (ICML-11), pages 1017–1024, 2011. \n\n  \n\n\\ [25\\] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to Sequence Learning with Neural Networks. InAdvancesin Neural Information Processing Systems (NIPS), pages 3104–3112, 2014. \n\n  \n\n\\ [26\\] The Theano Development Team. Theano: A Python framework for fast computation of mathematicalexpressions, 2016. \n\n  \n\n\\ [27\\] M. Wright and A. Freed. Open Sound Control: A new protocol for communicating with sound synthesizers.InProceedings of the 1997 International Computer Music Conference (ICMC), 1997. \n\n  \n\n\\ [28\\] Z. Wu and S. King. Investigating Gated Recurrent Neural Networks for Speech Synthesis. InIEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5140–5144. IEEE,2016. \n\n  \n\n\\ [29\\] W. Zaremba and I. Sutskever. Learning to Execute. In2nd International Conference on LearningRepresentations (ICLR2014), 2014.\n\n  \n\n#biblio","hData":{"t":"References\n==========\n\n  \n\n [1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult.IEEE Transactions on Neural Networks, 5(2):157–166, 1994.\n\n  \n\n [2] N. Boulanger-Lewandowski, P. Vincent, and Y. Bengio. Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription.arXiv preprintarXiv:1206.6392, 2012.\n\n  \n\n [3] F. Chollet. Keras: Deep Learning library for Theano and TensorFlow, 2015.\n\n  \n\n [4] L. Crnkovic-friis and L. Crnkovic-friis. Generative Choreography using Deep Learning.arXiv preprintarXiv:1605.06921, 2016.\n\n  \n\n [5] T. G. Dietterich. Ensemble Methods in Machine Learning.MCS ’00: Proceedings of the First InternationalWorkshop on Multiple Classifier Systems, pages 1–15, 2000.\n\n  \n\n [6] D. Eck and J. Schmidhuber. A First Look at Music Composition using LSTM Recurrent Neural Networks.Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale, 103, 2002.\n\n  \n\n\\ [7\\] F. A. Gers and J. Schmidhuber. Recurrent nets that time and count. InNeural Networks, 2000. IJCNN2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, pages 189–194. IEEE,2000.\n\n  \n\n\\ [8\\] R. Goodwin. Word Synth, 2016. \n\n  \n\n\\ [9\\] A. Graves. Sequence transduction with recurrent neural networks.arXiv preprint arXiv:1211.3711, 2012. \n\n  \n\n\\ [10\\] A. Graves. Generating sequences with Recurrent Neural Networks.arXiv preprint arXiv:1308.0850, 2013. \n\n  \n\n\\ [11\\] A. Graves, M. Liwicki, S. Fernández, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel connectionistsystem for unconstrained handwriting recognition.IEEE Transactions on Pattern Analysis and MachineIntelligence, 31(5):855–868, 2009. \n\n  \n\n\\ [12\\] K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, and J. Schmidhuber. LSTM: A Search SpaceOdyssey.arXiv preprint arXiv:1503.04069, page 10, 2015. \n\n  \n\n\\ [13\\] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. DRAW: A Recurrent Neural Network For ImageGeneration.arXiv preprint arXiv:1502.04623, 2015. \n\n  \n\n\\ [14\\] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,T. N. Sainath, and B. Kingsbury. Deep Neural Networks for Acoustic Modeling in Speech Recognition.IEEE Signal Processing Magazine, 29(6):82–97, 2012. \n\n  \n\n\\ [15\\] S. Hochreiter.Untersuchungen zu dynamischen neuronalen Netzen. PhD thesis, Technische UniversitätMünchen, 1991. \n\n  \n\n\\ [16\\] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory.Neural Computation, 9(8):1735–1780,1997. \n\n  \n\n\\ [17\\] A. Karpathy. The Unreasonable Effectiveness of Recurrent Neural Networks, 2015. \n\n  \n\n\\ [18\\] Z. Lieberman, T. Watson, and A. Castro. OpenFrameworks, 2016. \n\n  \n\n\\ [19\\] A. Nayebi and M. Vitelli. GRUV : Algorithmic Music Generation using Recurrent Neural Networks. 2015. \n\n  \n\n\\ [20\\] V. Pham, T. Bluche, C. Kermorvant, and J. Louradour. Dropout Improves Recurrent Neural Networks forHandwriting Recognition. InFrontiers in Handwriting Recognition (ICFHR), 2014 14th InternationalConference on, pages 285–290. IEEE, 2014. \n\n  \n\n\\ [21\\] J. Schmidhuber. Deep Learning in Neural Networks: An Overview.Neural Networks, 61:85–117, 2015. \n\n  \n\n\\ [22\\] B. Sturm. Recurrent Neural Networks for Folk Music Generation, 2015. \n\n  \n\n\\ [23\\] I. Sutskever.Training Recurrent neural Networks. PhD thesis, University of Toronto, 2013. \n\n  \n\n\\ [24\\] I. Sutskever, J. Martens, and G. Hinton. Generating Text with Recurrent Neural Networks. InProceedingsof the 28th International Conference on Machine Learning (ICML-11), pages 1017–1024, 2011. \n\n  \n\n\\ [25\\] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to Sequence Learning with Neural Networks. InAdvancesin Neural Information Processing Systems (NIPS), pages 3104–3112, 2014. \n\n  \n\n\\ [26\\] The Theano Development Team. Theano: A Python framework for fast computation of mathematicalexpressions, 2016. \n\n  \n\n\\ [27\\] M. Wright and A. Freed. Open Sound Control: A new protocol for communicating with sound synthesizers.InProceedings of the 1997 International Computer Music Conference (ICMC), 1997. \n\n  \n\n\\ [28\\] Z. Wu and S. King. Investigating Gated Recurrent Neural Networks for Speech Synthesis. InIEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5140–5144. IEEE,2016. \n\n  \n\n\\ [29\\] W. Zaremba and I. Sutskever. Learning to Execute. In2nd International Conference on LearningRepresentations (ICLR2014), 2014.\n\n  \n\n","y":"biblio"},"id":"125YxccQhU","created":"2020-10-06T02:09:40.610Z","color":"red","guid":"12cfpcfSFz"},"10":{"type":"text","rendermd":" [...] with increased compute power and large training sets, LSTMs and related architectures are proving successful not only in sequence classification [11, 14, 20, 12] , but also in sequence generation in many domains such as music [6, 2, 19, 22] , text [24, 23] , handwriting [10] , images [13] , machine translation \\ [25\\] , speech synthesis \\ [28\\] and even choreography \\ [4\\] \n\n  \n\n#review","hData":{"t":" [...] with increased compute power and large training sets, LSTMs and related architectures are proving successful not only in sequence classification [11, 14, 20, 12] , but also in sequence generation in many domains such as music [6, 2, 19, 22] , text [24, 23] , handwriting [10] , images [13] , machine translation \\ [25\\] , speech synthesis \\ [28\\] and even choreography \\ [4\\] \n\n  \n\n","y":"review"},"id":"121ucAm6J7","created":"2020-10-06T02:11:38.318Z","color":"#9900EF","guid":"122DedM4xS"},"11":{"type":"text","rendermd":" [...] most current applications of sequence generation with RNNs is not a real-time, interactive process.\n\n  \n\n#problematic","hData":{"t":" [...] most current applications of sequence generation with RNNs is not a real-time, interactive process.\n\n  \n\n","y":"problematic"},"id":"12TEgohxPk","created":"2020-10-06T02:11:50.700Z","color":"#FF6900","guid":"1S1Xz84FEt"},"12":{"type":"text","rendermd":"ll does not provide real-time continuous control in the manner required for the creation of expressive interface\n\n  \n\n#problematic","hData":{"t":"ll does not provide real-time continuous control in the manner required for the creation of expressive interface\n\n  \n\n","y":"problematic"},"id":"12mSqHw9VN","created":"2020-10-06T02:14:25.537Z","color":"#FF6900","guid":"1Sah8mKTCH"}}}